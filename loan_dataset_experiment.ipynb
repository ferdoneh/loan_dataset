{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferdoneh/loan_dataset/blob/main/loan_dataset_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z63mVVusKpCQ"
      },
      "source": [
        "# **Data Exploration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIrso2Hq-EwV",
        "outputId": "f7d402fa-be76-4dd3-e5d2-c2291ee6b077"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement sklearn.model_selection (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for sklearn.model_selection\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fingertips_py in /usr/local/lib/python3.8/dist-packages (0.2.2)\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from fingertips_py) (1.3.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fingertips_py) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.18.1->fingertips_py) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.18.1->fingertips_py) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.18.1->fingertips_py) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fingertips_py) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fingertips_py) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fingertips_py) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fingertips_py) (4.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=0.18.1->fingertips_py) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "! pip install sklearn.model_selection \n",
        "import numpy as np\n",
        "import re\n",
        "import datetime\n",
        "from math import sqrt\n",
        "\n",
        "import cufflinks as cf\n",
        "import plotly.graph_objs as go\n",
        "! pip install fingertips_py\n",
        "\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn import preprocessing\n",
        "import csv\n",
        "import chardet\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from collections import Counter\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWQKqV8SfsxV",
        "outputId": "9431bf46-cba7-493a-8be3-dab7eee0b116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement encoding (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for encoding\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "52Kw2ljVZLYa"
      },
      "outputs": [],
      "source": [
        "import fingertips_py as ftp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TDcCi-laX5C",
        "outputId": "ab6f15ca-940b-4d2f-8185-183ebbf2ba5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 0: syntax error near unexpected token `*fingertips_py'\n",
            "/bin/bash: -c: line 0: `help(*fingertips_py function name*)'\n"
          ]
        }
      ],
      "source": [
        "!help(*fingertips_py function name*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OnO8l_OvbzFQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "3c1d33c5-5c19-4260-dcaf-fbf3375cecd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'loan_dataset' already exists and is not an empty directory.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-44fe1b50cce7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'git clone https://github.com/ferdoneh/loan_dataset.git'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/loan_data_set.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/loan_data_set.csv'"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ferdoneh/loan_dataset.git\n",
        "df = pd.read_csv(\"/content/loan_data_set.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiAYhDq7oH5T"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqGa3NQjEDdp"
      },
      "outputs": [],
      "source": [
        "#Check the number of columns and row the dataset has.\n",
        "print(f'The dataset has {df.shape[1]} columns and {df.shape[0]} rows.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acQ3cEVPELxf"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZpvELyO2Rfu"
      },
      "outputs": [],
      "source": [
        " #Print unique values less than 13.\n",
        "for i in df.columns:\n",
        "    if len(df[i].unique()) < 100:\n",
        "        #Print features with discrete values.\n",
        "        print(f'{i} has values {df[i].unique()}, {len(df[i].unique())} values total.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vV8RnwnMtFY"
      },
      "source": [
        "**Features Checking**\n",
        "\n",
        "The labels of the features to see that they are all present\n",
        "change label names for  the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gJv3MMn3w11"
      },
      "outputs": [],
      "source": [
        "label = df['Loan_Status']\n",
        "df = df.drop(columns='Loan_Status')\n",
        "df[\"Label\"] = label\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwm9Y1RHMdzD"
      },
      "outputs": [],
      "source": [
        "df[\"Label\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8_eMP4LpVL4"
      },
      "outputs": [],
      "source": [
        "df['Gender'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-S1K8sMfFZAE"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUaalgzo0Oy7"
      },
      "outputs": [],
      "source": [
        "label = df.iloc[: , -1]\n",
        "print(label)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gf2j0ZFtfm6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VQfoFhx9aJE"
      },
      "source": [
        "**Class Distribution Check the class distribution for imbalance**\n",
        "The code below shows the percentage of Mycobacterium which resistance toward the antibiotics , it is displaying the data *is imbalanced *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3ZS4HnA9bsl"
      },
      "outputs": [],
      "source": [
        "df[\"Label\"].unique()\n",
        "Yes = len(df[df.Label == 'Y'])\n",
        "No = len(df[df.Label == 'N'])                             \n",
        "print(f'{round(Yes/df.shape[0] * 100, 2)}% of Yes.')\n",
        "print(f'{round(No/ df.shape[0] * 100, 2)}% of No.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NuPNpZDhu2V"
      },
      "source": [
        "**Finding the percentage of missing values in the datasete**\n",
        "\n",
        "Missing Value Check for missing values to see if they need to be filled in as found in the data exploration of the dataset , most of columns have a large amount of missing data. however dropping them from the dataset or even filled them with mean value won't help us to find optimal prediction "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kqdt7ZfaZvW"
      },
      "outputs": [],
      "source": [
        "def missing_zero_values_table(df):\n",
        "        zero_val = (df == 0.00).astype(int).sum(axis=0)\n",
        "        mis_val = df.isnull().sum()\n",
        "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
        "        mz_table = pd.concat([zero_val, mis_val, mis_val_percent], axis=1)\n",
        "        mz_table = mz_table.rename(\n",
        "        columns = {0 : 'Zero Values', 1 : 'Missing Values', 2 : '% of Total Values'})\n",
        "        mz_table['Total Zero Missing Values'] = mz_table['Zero Values'] + mz_table['Missing Values']\n",
        "        mz_table['% Total Zero Missing Values'] = 100 * mz_table['Total Zero Missing Values'] / len(df)\n",
        "        mz_table['Data Type'] = df.dtypes\n",
        "        mz_table = mz_table[mz_table.iloc[:,1] != 0].sort_values('% of Total Values', ascending=False).round(1)\n",
        "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns and \" + str(df.shape[0]) + \" Rows.\\n\"      \n",
        "            \"There are \" + str(mz_table.shape[0]) +\" columns that have missing values.\")\n",
        "#         mz_table.to_excel('D:/sampledata/missing_and_zero_values.xlsx', freeze_panes=(1,0), index = False)\n",
        "        return mz_table\n",
        "\n",
        "missing_zero_values_table(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpGrL4_ARtGA"
      },
      "outputs": [],
      "source": [
        "# df.drop(['Genome ID'], axis = 1, inplace = True)\n",
        " #Check to see if Id has been removed. df.columns\n",
        "# df.drop(['Serovar','Biovar','MLST','Pathovar','Altitude'], axis = 1, inplace = True)\n",
        "# df.drop(['Latitude','Longitude', 'Other Environmental', 'Depth','Body Sample Subsite','Reference'], axis = 1, inplace = True)\n",
        "# df.drop(['PATRIC CDS'], axis = 1, inplace = True)\n",
        "# df.drop(['NCBI Taxon ID', 'Host Name'], axis = 1, inplace = True)\n",
        "#list_of_features = [\"Geographic Group\t\",\"Collection Year\",\"Laboratory Typing Method\",\"SRA Accession\",\"Geographic Location\",\"\",,,,,,,,,]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIaI1-3VlXhD"
      },
      "source": [
        "case deleting , "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRrRtj7Fle9V"
      },
      "source": [
        "missing data imputation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZurTqSsLleo0"
      },
      "source": [
        "\n",
        "SOM (Self-Organizing Maps) : is a type of artifi cial neural networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uil-s8O5tM6T"
      },
      "source": [
        "# **Feature Selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AtWJR_fgM3z"
      },
      "source": [
        "Putting the unique features into a small datafram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOGiZtJxafnY"
      },
      "outputs": [],
      "source": [
        "# dataFrame = pd.DataFrame(\n",
        "#    {\n",
        "#       \"Size\",\"Contigs\",\"Optimal Temperature\",\"Temperature Range\",\"Geographic Group\",\"Antibiotic\",\"Host Health\",\"Species\",\"Host Gender\",\n",
        "#                          \"Host Age\",\"Host Health\",\"Gram Stain\",\"Cell Shape\",\"Geographic Location\",\"Oxygen Requirement\",\"Label\"\n",
        "#    }\n",
        "# )\n",
        "# cols = [\"Size\",\"Contigs\",\"Optimal Temperature\",\"Temperature Range\",\"Geographic Group\",\"Antibiotic\",\"Host Health\",\"Species\",\"Host Gender\",\n",
        "#                          \"Host Age\",\"Host Health\",\"Gram Stain\",\"Cell Shape\",\"Geographic Location\",\"Oxygen Requirement\",\"Label\"]\n",
        "# df3 = df.copy()\n",
        "# for i in df3.columns:\n",
        "#   if i not in cols:\n",
        "#     df3.drop(columns=i, inplace=True)\n",
        "\n",
        "# df3.head()\n",
        "#featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
        "#featureScores.columns = ['Size','Chromosome','Plasmids','Contigs','Optimal Temperature',\"Temperature Range\",'Geographic Group','Antibiotic','Host Health','Species','Host Gender','Host Age','Host Health','Gram Stain','Cell Shape','Geographic Location','Oxygen Requirement','Label'] \n",
        " #naming the dataframe columns\n",
        "#print(featureScores.nlargest(10,'Score'))\n",
        "\n",
        "\n",
        "#DF = df['Size','Chromosome','Plasmids','Contigs','Optimal Temperature',\"Temperature Range\",'Geographic Group','Antibiotic','Host Health','Species','Host Gender','Host Age','Host Health','Gram Stain','Cell Shape','Geographic Location','Oxygen Requirement','Label']\n",
        "#NDF = pd.DataFrame(newselect)\n",
        "#NDF.head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qo7En7tao6UF"
      },
      "outputs": [],
      "source": [
        "#  #Print unique values less than 13.\n",
        "# for i in df3.columns:\n",
        "#     if len(df3[i].unique()) < 100:\n",
        "#         #Print features with discrete values.\n",
        "#         print(f'{i} has values {df3[i].unique()}, {len(df3[i].unique())} values total.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m320y25xBc9y"
      },
      "outputs": [],
      "source": [
        "# dataFrame.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI1xGpwUMEzk"
      },
      "source": [
        "# ***Changing Lable to binary ***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rscMmAvnMR88"
      },
      "source": [
        "The lable of the dataset has the value of 'Resistant' and 'Susceptible' .it would be necessary to change them to the value of 0 for 'Resistant' and 1 for 'Susceptible'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ty45QeUTMHDj"
      },
      "outputs": [],
      "source": [
        "# creating a function to change the lable to a binary 0 and 1 \n",
        "def Lablechange(Lable):\n",
        "  if Lable == 'Yes':\n",
        "    return 1 \n",
        "  elif Lable == 'No':\n",
        "    return 0\n",
        "  else :\n",
        "    return None\n",
        "    \n",
        "    df3[\"Label\"] = df3[\"Label\"].apply(Lablechange)\n",
        "    newDf[\"Label\"] = newDf[\"Label\"].apply(Lablechange)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96FP9DHwMYjE"
      },
      "outputs": [],
      "source": [
        "# hgs = df3['Host Gender'].to_list()\n",
        "# def Host_GenderChanges(hgs):\n",
        "#   if hgs == 'Male' or hgs == 'male' or hgs == 'M':\n",
        "#     return 'Male'\n",
        "#   elif hgs == 'Female' or hgs == 'female' or hgs == 'F':\n",
        "#     return 'Female'\n",
        "#   else :\n",
        "#     return 'Unknown'\n",
        "\n",
        "# applying the change to the gender coloumn\n",
        "df3['Host Gender'] = df3['Host Gender'].apply(Host_GenderChanges) \n",
        "\n",
        "Gender = df3['Host Gender']\n",
        "\n",
        "# creating a function to change the 'Host Gender' to a binary 0 and 1 \n",
        "\n",
        "def Host_GenderChanges(Gender):\n",
        "  if Gender == 'Male':\n",
        "    return 0 \n",
        "  elif Gender == 'Female':\n",
        "    return 1\n",
        "  elif Gender == 'nan':\n",
        "    return 2\n",
        "  else :\n",
        "    return Non\n",
        "Gender = df3['Host Gender'].apply(Host_GenderChanges)\n",
        "Gender.unique()\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd5JQJbgHYAf"
      },
      "source": [
        "# **Visualise frequency distribution.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhLx1Kq9quQj"
      },
      "outputs": [],
      "source": [
        "df3['Host Age'].value_counts()\n",
        "# df['Host Age New'] = ['<18'  in x else '>=18 for x in idx]\n",
        "idx =[]\n",
        "for idx, i in enumerate(df3['Host Age']):\n",
        "  if df3['Host Age'][idx] != \"nan\":\n",
        "    if type(df3['Host Age'][idx]) != str:\n",
        "      print(type(df3['Host Age'][idx]))\n",
        "      if df3['Host Age'][idx] >=18:\n",
        "        df3['Host Age'][idx] = '>=18'\n",
        "      elif df3['Host Age'][idx] <18:\n",
        "        df3['Host Age'][idx] = '<18'\n",
        "        print(\"HIIIIIIIII\")\n",
        "    elif type(df3['Host Age'][idx]) == str:\n",
        "      if \"month\" in df3['Host Age'][idx]:\n",
        "        df3['Host Age'][idx] = '<18'\n",
        "      elif '<18' in df3['Host Age'][idx]:\n",
        "          df3['Host Age'][idx] = '<18'\n",
        "      # elif df['Host Age'][idx] == \">=18\":\n",
        "      else:\n",
        "        df3['Host Age'][idx] = '>=18'\n",
        "      # elif \"month\" not in df['Host Age'][idx]:\n",
        "      #   df['Host Age'][idx] = \">=18\"\n",
        "\n",
        "  else:\n",
        "    continue\n",
        "\n",
        "\n",
        "      # elif df['Host Age'][idx] == \"Adult\":\n",
        "      #   df['Host Age'][idx] = \">=18\"\n",
        "      # elif df['Host Age'][idx] == \"33y\":\n",
        "      #   df['Host Age'][idx] = \">=18\"\n",
        "      # elif df['Host Age'][idx] == \"4 months\":\n",
        "      #   df['Host Age'][idx] = \"<18\"\n",
        "      # elif df['Host Age'][idx] == \"8 months 1o days\":\n",
        "      #   df['Host Age'][idx] = \"<18\"\n",
        "\n",
        "  \n",
        "  # s.rename({1: 3, 2: 5})\n",
        "# s = s.apply(lambda x: '>=18' if (x >= 18) else '<18')\n",
        "# s.unique()\n",
        "\n",
        "df3['Host Age'].value_counts()\n",
        "df3['Host Age'] = df3['Host Age'].replace('Adult','>=18')\n",
        "print(df3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4L7nBdqcWE4"
      },
      "outputs": [],
      "source": [
        "sns.set(rc={'figure.figsize':(9,5)})\n",
        "sns.countplot(data= df3, x='Host Age',hue='Label')\n",
        "plt.title('Age v/s Label\\n')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZDGeKgrHebN"
      },
      "outputs": [],
      "source": [
        "sns.set(rc={'figure.figsize':(9,5)})\n",
        "sns.countplot(data= df3, x='Size',hue='Label')\n",
        "plt.title('Size v/s Label\\n')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERE2DKR-TZvu"
      },
      "outputs": [],
      "source": [
        "sns.set(rc={'figure.figsize':(9,5)})\n",
        "sns.countplot(data= df3, x=df3['Host Gender'],hue='Label')\n",
        "plt.title('Gender v/s Label\\n')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nM7nC9qlQ50C"
      },
      "outputs": [],
      "source": [
        "sns.set(rc={'figure.figsize':(9,5)})\n",
        "sns.countplot(data= df3, x=df3['Host Health'],hue='Label')\n",
        "plt.title('Host Health v/s Label\\n')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bs8jHeH_tZdv"
      },
      "outputs": [],
      "source": [
        "df3[\"Geographic Location\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GykI_z-yqi2y"
      },
      "outputs": [],
      "source": [
        "sns.set_style('darkgrid')\n",
        "sns.set(rc = {'figure.figsize' : (50, 5)})\n",
        "#Make 2 bar graphs in one go.\n",
        "fig, axs = plt.subplots(ncols=2)   \n",
        "sns.countplot(x = df3[\"Geographic Location\"], data = df3, ax = axs[0])\n",
        "sns.countplot(x = df3['Contigs'], data = df3, ax = axs[1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BimtMFE8iCa"
      },
      "source": [
        "**Check correlation between some of the features**\n",
        "\n",
        "The term \"correlation\" refers to the statistical association between two variables. A correlation could be positive or negative, indicating that when one variable's value changes, the other variables' values change in the opposite manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOLvSeKDyiiX"
      },
      "outputs": [],
      "source": [
        "# plot a correlation matrix to check correlations between variables\n",
        "##Visualize correlation between features.\n",
        "ax = plt.figure(figsize=(13,5))\n",
        "corrMatrix = df3.corr()\n",
        "print(corrMatrix)\n",
        "\n",
        "\n",
        "ax = sns.heatmap(corrMatrix, annot=True)\n",
        "plt.plot()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZNj7rcd-49l"
      },
      "source": [
        "# **As an learning documents**\n",
        "\n",
        "Algorithms that Support Missing Values like k-nearst neighbor and Naive Bayes is good option ,it can support missing values when making a prediction. we have a variety of possibilities to replacing a missing value, such as: Ignoring and Discarding Data (deleting instances and=or attributes) Parameter Estimation. Maximum likelihood procedures are used to estimate the parameters of a model constant value, such as 0, that is different from all other values. Imputation. Imputation is a class of procedures (replacing missing values with estimated ones) A value from a different record that was chosen at random. The column's mean, median, or mode value.[by the mean (if the attribute is quantitative) or mode (if the attribute is qualitative)]\n",
        "\n",
        "We conclude from the results that the performance of the classifiers and imputation strategies generally depend on the nature and proportion of missing data\n",
        "Double-click (or enter) to edit\n",
        "\n",
        "When ffill() is applied across the index then any missing value is filled based on the corresponding value in the previous row. but because the missing value is more than we expected ????????????:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3vBS_CX_F3d"
      },
      "outputs": [],
      "source": [
        "#newDf.columns\n",
        "#newDf['Antimicrobial Resistance Evidence'].unique()\n",
        "#df['Label'].value_counts()\n",
        "#newDf['Label'].value_counts()\n",
        "#newDf['Label'] = newDf['Label'].replace('Resistant;Susceptible','Resistant')\n",
        "#print(newDf)\n",
        "#newDf['Label'] = newDf['Label'].replace('Resistant;Susceptible','Resistant')\n",
        "#print(newDf)\n",
        "#df['Label'] = df['Label'].replace('Resistant;Susceptible','Resistant')\n",
        "#print(df)\n",
        "#df['Label'] = df['Label'].replace('Susceptible;Resistant','Susceptible')\n",
        "#print(df)\n",
        "# df3=newDf.drop([0], axis = 0)\n",
        "# print(df3)\n",
        "# df2=df.drop([0], axis = 0)\n",
        "# print(df2)\n",
        "\n",
        "# drop rows with missing values\n",
        "#newDf.dropna(inplace=True)\n",
        "#print(f'The dataset has {newDf.shape[1]} columns and {newDf.shape[0]} rows.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-vDSZBO461N"
      },
      "source": [
        "# ***Missing Data with Self organization map***\n",
        " SOM is a type of artificial neural network (ANN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZL5HlyKBNqo"
      },
      "source": [
        "**Normalise Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhhFPOr3kMeg"
      },
      "outputs": [],
      "source": [
        "!pip install minisom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqLt7ydf55b-"
      },
      "outputs": [],
      "source": [
        "df3.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjGWTWBH9Qke"
      },
      "outputs": [],
      "source": [
        "df3.Label.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ig88l9wfA_M_"
      },
      "outputs": [],
      "source": [
        "#Normalise X, input featurees.\n",
        " #Importing preprocessing and min-max feature scalling\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from minisom import MiniSom\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "# x = df3.values\n",
        "# scaler = MinMaxScaler()\n",
        "# X_scaler = scaler.fit_transform(X)\n",
        "# df3_1 = pd.DataFrame(x_scaled)\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# preprocessing_data = preprocessing.normalize(X)\n",
        "# print(preprocessing_dat)\n",
        "# dfx = df3.copy()\n",
        "# dfx = dfx.dropna()\n",
        "transformer = make_column_transformer(\n",
        "    (OneHotEncoder(sparse=False), ['Species','Geographic Group', 'Geographic Location',\n",
        "       'Host Gender', 'Host Age', 'Host Health', 'Antibiotic',\"Label\"]),\n",
        "    remainder='passthrough')\n",
        "\n",
        "transformed = transformer.fit_transform(df3)\n",
        "transformed_df = pd.DataFrame(transformed, columns=transformer.get_feature_names())\n",
        "transformed_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "455kIhJl-VQe"
      },
      "outputs": [],
      "source": [
        "transformed_df['Contigs'] = transformed_df['Contigs'].astype(float)\n",
        "transformed_df['Size'] = transformed_df['Size'].astype(float)\n",
        "transformed_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viQ7Aq96GGnk"
      },
      "outputs": [],
      "source": [
        "dfx = transformed_df.drop([\"onehotencoder__x7_Intermediate\",\n",
        "                           \"onehotencoder__x7_Resistant\",\n",
        "                           \"onehotencoder__x7_Susceptible\",\n",
        "                           \"onehotencoder__x7_nan\"] ,axis=1)\n",
        "\n",
        "\n",
        "dfx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2jv_lowGnGw"
      },
      "outputs": [],
      "source": [
        "dfy = transformed_df[[\"onehotencoder__x7_Intermediate\"\t,\"onehotencoder__x7_Resistant\"\t,\"onehotencoder__x7_Susceptible\",\t\"onehotencoder__x7_nan\"]]\n",
        "dfy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHdkzE70sMOM"
      },
      "outputs": [],
      "source": [
        " #Giving the columns identifiable names\n",
        "dfy = dfy.rename(columns={0:\"onehotencoder__x7_Intermediate\", 1:\"onehotencoder__x7_Resistant\", 2:\"onehotencoder__x7_Susceptible\", 3: \"onehotencoder__x7_nan\"})\n",
        "dfy.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0NSZPnK5JGV"
      },
      "outputs": [],
      "source": [
        "#divide data\n",
        "X = dfx.values\n",
        "y = dfy.values\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIP9PM2m5lS9"
      },
      "outputs": [],
      "source": [
        "#initializing self organizing map\n",
        "from minisom import MiniSom\n",
        "som = MiniSom(x=262 ,y=4,input_len=262,sigma=1.0,learning_rate=0.5)\n",
        "som.random_weights_init(X)\n",
        "som.train_random(data = X , num_iteration = 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pk3g1JsagsqL"
      },
      "outputs": [],
      "source": [
        " #visualizing the som\n",
        "from pylab import bone,pcolor,colorbar,plot,show\n",
        "bone()\n",
        "pcolor(som.distance_map().T)\n",
        "colorbar()\n",
        "markers = ['o','s']\n",
        "colors = ['r','g']\n",
        "for i,x in enumerate(X):\n",
        "    w = som.winner(x)\n",
        "    plot(w[0]+0.5,w[1]+0.5,markers[y[i]],\n",
        "         markeredgecolor=colors[y[i]],\n",
        "         markersize=10,\n",
        "         markeredgewidth=2,\n",
        "         markerfacecolor='None'\n",
        "         )\n",
        "show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2H7Q4BNhgxYg"
      },
      "outputs": [],
      "source": [
        "#finding the frauds\n",
        "mappings = som.win_map(X)\n",
        "frauds = np.concatenate((mappings[(1,1)],mappings[(1,2)]),axis=0)\n",
        "# frauds = sc.inverse_transform(frauds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Necys0SBJD3D"
      },
      "outputs": [],
      "source": [
        "frauds.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0ygZ0DCJSke"
      },
      "outputs": [],
      "source": [
        "dfx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44F9Zu09sSs7"
      },
      "outputs": [],
      "source": [
        "# Recursive Feature Elimination\n",
        "# create a base classifier used to evaluate a subset of attributes\n",
        "model = LogisticRegression()\n",
        "# create the RFE model and select 3 attributes\n",
        "rfe = RFE(model, 12)\n",
        "rfe = rfe.fit(df.data, df.target)\n",
        "# summarize the selection of the attributes\n",
        "print(rfe.support_)\n",
        "print(rfe.ranking_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTNw3_Xi1YyF"
      },
      "source": [
        "# **Missing Data with Imputation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaLowSC_16s8"
      },
      "outputs": [],
      "source": [
        "#SIMPLE Imputation\n",
        "from sklearn.impute import SimpleImputer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Specify the strategy to be the median class\n",
        "fea_transformer = SimpleImputer(strategy=\"median\")\n",
        "values = fea_transformer.fit_transform(transformed_df)\n",
        "pd.DataFrame(values)"
      ],
      "metadata": {
        "id": "47-U8qaBkoQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d-kPI9L2PwV"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "# I specify the nearest neighbor to be 3 \n",
        "fea_transformer = KNNImputer(n_neighbors=3)\n",
        "values = fea_transformer.fit_transform(transformed_df)\n",
        "pd.DataFrame(values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tu4MDWnDwt6"
      },
      "source": [
        "**Editing Features**\n",
        "\n",
        "The unique values that are not mentioned in the dataset description will be moved into the “Other” value, since they appear very infrequently in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxmlw3jNEHYw"
      },
      "outputs": [],
      "source": [
        "#Change categorical values by replacing them.\n",
        "# df['Host Gender'].replace([0,1],2, inplace=True) \n",
        "#4 = Other\n",
        "#df[\"Marriage\"].replace(0, 3, inplace=True) #3 = Other\n",
        "#Check outcome.\n",
        "#print(f'\"Host Age\" has discrete values of {df[\"Host Age\"].unique()}, {len(df[\"Host Age\"].unique())} values in total.')\n",
        "#print(f'Marriage has discrete values of {df[\"Marriage\"].unique()}, {len(df[\"Marriage\"].unique())} values in total.')\n",
        "#Categorical values.\n",
        "#category = ['Sex', \"Host Age\", 'Marriage','PayStat_April']\n",
        "#Create dataframe for each categorical feature.\n",
        "#sex = pd.get_dummies(df['Sex'], prefix = 'Sex')\n",
        "#education = pd.get_dummies(df[\"Host Age\"], prefix = \"Host Age\")\n",
        "#marriage = pd.get_dummies(df['Marriage'], prefix = 'Marriage')\n",
        "#september = pd.get_dummies(df['PayStat_September'], prefix = 'PayStat_September') august = pd.get_dummies(df['PayStat_August'], prefix = 'PayStat_August')\n",
        "#july = pd.get_dummies(df['PayStat_July'], prefix = 'PayStat_July')\n",
        "#Remove original features.\n",
        "#df = df.drop(category, axis = 1)\n",
        "#Add the new dataframes into the original dataframe.\n",
        "#df_tmp = [df, sex, education, marriage, september, august, july, june, may, april] df = pd.concat(df_tmp, axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zektxTc7EeRV"
      },
      "source": [
        "Check new features that were added.\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYOdNcdqEmFS"
      },
      "source": [
        "# **Sampling Methods**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i12SaqspEdWC"
      },
      "outputs": [],
      "source": [
        " #Function to cluster dataframe and returned sampled dataframe.\n",
        "#def cluster(clusters, dataframe, fraction, replace):\n",
        "#Cluster dataframe.\n",
        "#kmeans = KMeans(n_clusters = clusters, random_state = 1906814).fit(dataframe); \n",
        "#dataframe['Cluster'] = kmeans.predict(dataframe)\n",
        "#Empty dataframe to store sampled data.\n",
        "#clusters_df = pd.DataFrame()\n",
        "#Sample each cluster and add it to the empty dataframe.\n",
        "#for i in range(clusters):\n",
        "#c = dataframe[dataframe.Cluster == i].copy(); \n",
        "#c = c.sample(frac = fraction, random_state = 1906814, replace = replace)\n",
        "#clusters_df = pd.concat([c, clusters_df])\n",
        "#Drop the Cluster column.\n",
        "#clusters_df.drop(['Cluster'], axis = 1, inplace = True) #Return sampled dataframe\n",
        "    #return clusters_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPTBqFJNGsOs"
      },
      "source": [
        "we use the randome_state to getsame out come or result from  processing of same model . using the randome_state provide same result "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAOL46e_Ez1v"
      },
      "source": [
        "## **Prepare Features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2zDZeD8E3x5"
      },
      "outputs": [],
      "source": [
        "#Store default feature.\n",
        "Y = df[\"Label\"]\n",
        "#Store the rest of the features.\n",
        "X = small_df. drop(\"Label\", axis = 1) \n",
        "#Check X and Y data types.\n",
        "print(type(X))\n",
        "print(type(Y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3qkQ5KeNmDP"
      },
      "source": [
        "**Note that we cannot use a dataset with NaN values for k-fold cross validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYTFAjVmFSE8"
      },
      "source": [
        "# **Split the Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGinS7flFUlu"
      },
      "source": [
        "\n",
        "here is no best value for seed. It depends on the data\n",
        "#Split X and Y into testing and training set.\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state = 10116) \n",
        "different random state then mean_squared_error will be different every time.\n",
        "in randomforest using the randome-state provide when building trees and the sampling of the features to consider when looking for the best split at each node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqEWc2vV8X87"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state = 10116)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LgW-jBLFdYK"
      },
      "source": [
        "# **Modelling / Classification**\n",
        "\n",
        "\n",
        "*  Build Model\n",
        "*  Create and Train Random Forest\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAo648FWF0Mv"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn import svm\n",
        "from sklearn import LogisticRegressionclassifier \n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_classification\n",
        " #Create RandomForestClassifier with default parameters.\n",
        "RF = RandomForestClassifier(random_state = 10116 )\n",
        " #Fit the training data into the model. \n",
        "RF.fit(X_train, Y_train)\n",
        "RandomForestClassifier(random_state=10116 )\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWbTeGZIGGh7"
      },
      "source": [
        "# **Make Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mESxGrNMGQmw"
      },
      "outputs": [],
      "source": [
        " #Predict using testing set.\n",
        "rf_predict = rf.predict(X_test)\n",
        " #Prediction probabilities.\n",
        "rf_probs = rf.predict_proba(X_test)\n",
        "clf = RandomForestClassifier(max_depth=2, random_state=10116)\n",
        "clf.fit(X, y)\n",
        "RandomForestClassifier(...)\n",
        "print(clf.predict([[0, 0, 0, 0]]))\n",
        "[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jcd0FM27_b8u"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "names = [\"Decision Tree\", \"Random Forest\", \"Neural Net\",\"SVM\" , \"AdaBoostClassifier\", \"GradientBoostingClassifier\" ]\n",
        "\n",
        "classifiers = [ DecisionTreeClassifier(),RandomForestClassifier() , MLPClassifier()]\n",
        "\n",
        "score = 0\n",
        "for name, clf in zip(names, classifiers):\n",
        "    if name == \"Decision Tree\":\n",
        "        clf = DecisionTreeClassifier(random_state=0)\n",
        "        grid_search = GridSearchCV(clf, param_grid=param_grid_DT)\n",
        "        grid_search.fit(X_train, y_train_TF)\n",
        "        if grid_search.best_score_ > score:\n",
        "            score = grid_search.best_score_\n",
        "            best_clf = clf\n",
        "    elif name == \"Random Forest\":\n",
        "        clf = RandomForestClassifier(random_state=0)\n",
        "        grid_search = GridSearchCV(clf, param_grid_RF)\n",
        "        grid_search.fit(X_train, y_train_TF)\n",
        "        if grid_search.best_score_ > score:\n",
        "            score = grid_search.best_score_\n",
        "            best_clf = clf\n",
        "\n",
        "    elif name == \"Neural Net\":\n",
        "        clf = MLPClassifier()\n",
        "        clf.fit(X_train, y_train_TF)\n",
        "        y_pred = clf.predict(X_test)\n",
        "        current_score = accuracy_score(y_test_TF, y_pred)\n",
        "        if current_score > score:\n",
        "            score = current_score\n",
        "            best_clf = clf\n",
        "\n",
        "    elif name == \"SVM\":\n",
        "        clf = SVM()\n",
        "        clf.fit(X_train, y_train_TF)\n",
        "        y_pred = clf.predict(X_test)\n",
        "        current_score = accuracy_score(y_test_TF, y_pred)\n",
        "        if current_score > score:\n",
        "            score = current_score\n",
        "            best_clf = clf\n",
        "\n",
        "\n",
        "pkl_filename = \"pickle_model.pkl\"  \n",
        "with open(pkl_filename, 'wb') as file:  \n",
        "    pickle.dump(best_clf, file)\n",
        "\n",
        "from sklearn.externals import joblib\n",
        "# Save to file in the current working directory\n",
        "joblib_file = \"joblib_model.pkl\"  \n",
        "joblib.dump(best_clf, joblib_file)\n",
        "\n",
        "print(\"best classifier: \", best_clf, \" Accuracy= \", score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCGPdmfiGUqq"
      },
      "source": [
        "# **Results**\n",
        "Get the overall accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjYTN_hUGkxC"
      },
      "outputs": [],
      "source": [
        " #Get model accuracy\n",
        "accuracy = accuracy_score(Y_test, rf_predict)\n",
        "#Print accuracy\n",
        "print(f'The accuracy of the Random Forest model is {np.round(accuracy * 100, 2)}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4m5sDQXcGoYy"
      },
      "source": [
        "# **Evaluate Model**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_aAqA0qGyC5"
      },
      "source": [
        "#Show classification report.\n",
        "print(classification_report(Y_test, rf_predict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzZ13zZUG5uC"
      },
      "outputs": [],
      "source": [
        " #Create a heatmap for the confusion matrix\n",
        "heat_map = sns.heatmap(confusion_matrix(Y_test, rf_predict), annot = True, fmt = \"d\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wA7iaAvG-yr"
      },
      "source": [
        "\n",
        "## **Results Analysis**\n",
        "# **Solution Improvement**\n",
        "**Tune Parameters**\n",
        "When building the Random Forest Classifier it can be given parameters and adjusting these pa- rameters can lead to improvement in performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89fhqPffHg5B"
      },
      "outputs": [],
      "source": [
        "#  accuracy_list = []\n",
        "# for i in range(1, 1001):\n",
        "#     #Make random forest.\n",
        "# rf = RandomForestClassifier(random_state = 1906814, n_estimators = i)\n",
        "#  #Store default feature.\n",
        "# Y = df['Label']\n",
        "# #Store the rest of the features.\n",
        "# X = df.drop('Label', axis = 1)\n",
        "# #Normalize X, input features.\n",
        "# X = preprocessing.normalize(X)\n",
        "# #Split X and Y into testing and training set.\n",
        "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state =␣\n",
        "# 􏰀→1906814)\n",
        "# #Fit the training data into the model. rf.fit(X_train, Y_train)\n",
        "# #Predict using testing set.\n",
        "# rf_predict = rf.predict(X_test)\n",
        "# #Get model accuracy\n",
        "# accuracy = accuracy_score(Y_test, rf_predict) #Add accuracy to the list. accuracy_list.append(accuracy)\n",
        "# #Sort the list then get highest accuracy and number of estimators.\n",
        "# sort_list = sorted(accuracy_list, reverse = True)\n",
        "# estimators = accuracy_list.index(sort_list[0]) + 2\n",
        "# #Print\n",
        "# # print(\"Number of estimators: \" + str(estimators) + \" has highest accuracy of \" +␣\n",
        "# 􏰀→str(sort_list[0]) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_mLS1bgHsQU"
      },
      "source": [
        "## **Using Different Metrics for Evaluation**\n",
        "\n",
        "A function is created to easily predict the data and return the model’s accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJhyELtuH6cQ"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import mean_squared_error\n",
        "#Function to model the data.\n",
        "# def sampling(dataframe, testsize):\n",
        "# #Store default feature.\n",
        "# Y = dataframe['?????']\n",
        "# #Store the rest of the features.\n",
        "# X = dataframe.drop('???????', axis = 1)\n",
        "# #Normalize X, input features.\n",
        "# X = preprocessing.normalize(X)\n",
        "# #Split X and Y into testing and training set.\n",
        "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = testsize, random_state␣\n",
        "# 􏰀→= 1906814)\n",
        "# #Fit the training data into the model. rf.fit(X_train, Y_train)\n",
        "# #Predict using testing set.\n",
        "# rf_predict = rf.predict(X_test)\n",
        "# #Get model accuracy\n",
        "# accuracy = accuracy_score(Y_test, rf_predict) #Return accuracy\n",
        "\n",
        "# return np.round(accuracy * 100, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7WkGrmlH-zY"
      },
      "source": [
        "# **Undersampling**\n",
        "\n",
        "As discussed before undersampling will reduce the size of the no default records to match the default records. However, the cost of this method is that data is lost. The optimal number of clusters will be found, so that a higher accuracy for the model will be found. Range from 2 to 31 cluster size will be tested since the no default data is large."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_poMsNokII9v"
      },
      "outputs": [],
      "source": [
        "#  accuracy_list = []\n",
        "# for i in range(2, 31):\n",
        "#     #Make two dataframes, one each classification\n",
        "# default = df[df.???????? == 1].copy(); noDefault = df[df.?????????/ == 0].copy()\n",
        "#     #Sample the no default data and combine the two dataframes.\n",
        "# noDefault = cluster(i, noDefault, 0.33, False) undersampled_df = pd.concat([default, noDefault])\n",
        "#  #Add accuracy to the list. accuracy_list.append(sampling(undersampled_df, 0.25))\n",
        "# #Sort the list then get highest accuracy and number of clusters for it.\n",
        "# sort_list = sorted(accuracy_list, reverse = True)\n",
        "# number_of_clusters = accuracy_list.index(sort_list[0]) + 2\n",
        "# #Print\n",
        "# print(\"Number of cluster: \" + str(number_of_clusters) + \" has highest accuracy of \" +␣\n",
        "# 􏰀→str(sort_list[0]) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm12FU3TIl84"
      },
      "source": [
        "# **Oversampling**\n",
        "For oversampling the size of the default data will be increased to closely match the size of no default, ratio 1:1. Oversampling has the benefit of not losing data unlike undersampling, however the data will just be duplicated. The default data will be duplicat 2 times, so the size will be 3 times the original."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQExRvDDJB_E"
      },
      "outputs": [],
      "source": [
        " #Print accuracy\n",
        "print(f'The accuracy of the Random Forest model with oversampling is {sampling(oversampled_df, 0. 􏰀→25)}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziAZGAQ7JHOz"
      },
      "source": [
        "According to the results oversampling did improve the model, accuracy increased by 13%. There- fore in this scenario oversampling is better for the model’s accuracy than undersampling. Balanc- ing the data with oversampling is best."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRmS0n80JNHO"
      },
      "source": [
        "## **Combination**\n",
        "\n",
        "A combination of undersampling and oversampling may yield better results compared to only using oversampling. Again cluster size of 2 to 31 will used to find the best possible accuracy. The aim will be to keep dataset size the same (30000 records) while trying to have 1:1 ratio of classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdmtjg8LJa_M"
      },
      "outputs": [],
      "source": [
        "#  accuracy_list = []\n",
        "# for i in range(2, 31):\n",
        "#     #Make two dataframes, one each classification\n",
        "# default = df[df.Default_October == 1].copy(); 􏰀→copy()\n",
        "#     #Sample the no default default data.\n",
        "# noDefault = df[df.Default_October == 0].\n",
        "# noDefault = cluster(i, noDefault, 0.66, False) #Sample the no default default data.\n",
        "# oversample = cluster(i, default, 0.5, False) default = pd.concat([default, default, oversample]) default.drop(['Cluster'], axis = 1, inplace = True) #Combine dataframes.\n",
        "# combination_df = pd.concat([default, noDefault]) #Add accuracy to the list. accuracy_list.append(sampling(combination_df, 0.25))\n",
        "# #Sort the list then get highest accuracy and number of clusters for it.\n",
        "# sort_list = sorted(accuracy_list, reverse = True)\n",
        "# number_of_clusters = accuracy_list.index(sort_list[0]) + 2\n",
        "# #Print\n",
        "# print(\"Number of cluster: \" + str(number_of_clusters) + \" has highest accuracy of \" +␣\n",
        "# 􏰀→str(sort_list[0]) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFdhRiFwJgqe"
      },
      "source": [
        "# **Compare With Different Models**\n",
        "Even thought Random Forest is considered one of the best models, it necessarily might not be the best for this particular dataset. Therefore other classifiers will be built to compare with our current model. The some of the chosen classifiers are similar to the ones used in the peer-reviewed paper and the others are common classifiers. They will be compared not through accuracy but their f1-score and a ROC plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUsgRSIoJu6q"
      },
      "outputs": [],
      "source": [
        "#  #Store the classifiers\n",
        "# classifiers = [MLPClassifier(max_iter = 1000, random_state = 1906814),\n",
        "#                DecisionTreeClassifier(random_state = 1906814),\n",
        "#                AdaBoostClassifier(random_state = 1906814),\n",
        "#                GaussianNB(),\n",
        "#                rf\n",
        "# ]\n",
        "# #Store the model results into a dataframe.\n",
        "# results = pd.DataFrame(columns=['classifiers','fpr','tpr','auc']) #Train the models and store the results.\n",
        "# for cls in classifiers:\n",
        "# #Fit the data\n",
        "# model = cls.fit(X_train, Y_train)\n",
        "# #Get probabilites.\n",
        "# probability = model.predict_proba(X_test)[::,1]\n",
        "# #Set the columns.\n",
        "# fpr, tpr, thres = roc_curve(Y_test, probability)\n",
        "# #Get the area under the curve.\n",
        "# auc = roc_auc_score(Y_test, probability)\n",
        "# #Add the results to dataframe.\n",
        "# results = results.append({'classifiers':cls.__class__.__name__,\n",
        "# #Set names\n",
        "#   'fpr':fpr,\n",
        "#   'tpr':tpr,\n",
        "#   'auc':auc\n",
        "# }, ignore_index = True)\n",
        "# results.set_index('classifiers', inplace = True) #Create the plot\n",
        "# fig = plt.figure(figsize = (8, 6))\n",
        "# #Loop the results and plot the performance.\n",
        "# for i in results.index:\n",
        "#     plt.plot(results.loc[i]['fpr'],\n",
        "#              results.loc[i]['tpr'],\n",
        "# label=\"{}, AUC={:.3f}\".format(i, results.loc[i]['auc'])) #Plot the random model.\n",
        "# plt.plot([0,1], [0,1], color='orange', linestyle='--') #False Positive.\n",
        "# plt.xticks(np.arange(0.0, 1.1, step=0.1)) plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "# #True Positive.\n",
        "# plt.yticks(np.arange(0.0, 1.1, step=0.1)) plt.ylabel(\"True Positive Rate\", fontsize=15) #ROC Curve.\n",
        "# plt.title('ROC Curve Analysis', fontsize=12) plt.legend(prop={'size':13}, loc='lower right') #Show the plot.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG_V-fW0J0IY"
      },
      "source": [
        "According to the graph the AdaBoost classifier is the best model to use for this dataset. It is only slightly better than the Random Forest classifier, however with some tuning the Random Forest can perform better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ex9FaBTJ6ip"
      },
      "source": [
        "## **Change the Partitioning of the Data**\n",
        "\n",
        "Another way to improve the model is to use K fold cross-validation to get the average accuracy of the model. This method guarantees that all iterations of a split are tested. From 10% to 33% testing data split will be tested. The results will show the optimal training / testing size for the data, with over and under fitting taken into consideration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LMDcHrkKCel"
      },
      "outputs": [],
      "source": [
        "#  for i in range(3, 11):\n",
        "#     kfold = KFold(n_splits = i)\n",
        "#     score = cross_val_score(rf, X, Y, cv = kfold, scoring = \"accuracy\").mean()\n",
        "#     print(f'Using {100 / i}% for testing, mean accuracy: {round(score * 100, 2)}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-LJLPIaKIJI"
      },
      "source": [
        "The results show that from 10% to 33% testing size has around 81% accuracy. If the testing size is reduced further from 10% the model will have a High Variance (overfitting) and increasing the testing size above 33% will result in the model having a High Bias (Overfitting)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wVMjo1KKL4h"
      },
      "source": [
        "## **Final Model**\n",
        "\n",
        "based on most of the literature reviews , need to test various methods to improve the Random Forest’s performance. The following shows the best version of the model for this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJ7WMHAN9gMh"
      },
      "outputs": [],
      "source": [
        "corona = pd.read_excel(\"/content/20221202covid19infectionsurveydatasetsengland (1).xlsx\")\n",
        "corona.head()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "yYTFAjVmFSE8",
        "RCGPdmfiGUqq",
        "4m5sDQXcGoYy",
        "j_aAqA0qGyC5",
        "8wA7iaAvG-yr",
        "7_mLS1bgHsQU",
        "c7WkGrmlH-zY",
        "Bm12FU3TIl84",
        "XRmS0n80JNHO",
        "6Ex9FaBTJ6ip"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}